{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import xlrd, xlwt\n",
    "from tqdm import tqdm_notebook\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import csv\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ML moduls\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.utils import np_utils\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models import word2vec\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.data\n",
    "import logging\n",
    "import _pickle as cPickle\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Начальная обработка текста\n",
    "def review_to_wordlist(review, remove_stopwords=False):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    #\n",
    "    # 2. Remove non-letters\n",
    "    #review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Токенизация\n",
    "# Define a function to split a review into parsed sentences\n",
    "def review_to_sentences(review, tokenizer, remove_stopwords=False):\n",
    "    # Function to split a review into parsed sentences. Returns a\n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            list_of_words = review_to_wordlist(raw_sentence, remove_stopwords)\n",
    "            if len(list_of_words) > 0:\n",
    "                sentences.append(list_of_words)\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Составляем вектора\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    #\n",
    "    # Index2word is a list that contains the names of the words in\n",
    "    # the model's vocabulary. Convert it to a set, for speed\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    #\n",
    "    # Divide the result by the number of words to get the average\n",
    "    #print(featureVec)\n",
    "    if(nwords == 0):\n",
    "        global count_null_nwords \n",
    "        count_null_nwords = 0\n",
    "        count_null_nwords += 1\n",
    "        nwords += 1\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    #print(featureVec)\n",
    "    return featureVec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate\n",
    "    # the average feature vector for each one and return a 2D numpy array\n",
    "    #\n",
    "    # Initialize a counter\n",
    "    counter = 0\n",
    "    \n",
    "    #\n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    #\n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "       #\n",
    "       # Print a status message every 1000th review\n",
    "       #if (counter%1000.) == (0.):\n",
    "        #   print (\"Review %d of %d\" % (counter, len(reviews)))\n",
    "       #\n",
    "       # Call the function (defined above) that makes average feature vectors\n",
    "       reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "       \n",
    "       #\n",
    "       # Increment the counter\n",
    "       counter = counter + 1\n",
    "\n",
    "    return reviewFeatureVecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_writer(data, path):\n",
    "    with open(path, \"w\", newline='', encoding=\"utf8\") as csv_file:\n",
    "        writer = csv.writer(csv_file, delimiter='\\t')\n",
    "        writer.writerow(\"id_tweet, sentiment, us_id, followers\".split(\", \"))\n",
    "        for line in data:\n",
    "            writer.writerow(line)\n",
    "\n",
    "\n",
    "def csv_writer1(data, path):\n",
    "    with open(path, \"a\", newline='', encoding=\"utf8\") as csv_file:\n",
    "        writer = csv.writer(csv_file, delimiter='\\t')\n",
    "        for line in data:\n",
    "            writer.writerow(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lol():\n",
    "    path = \"C:/Users/Users/Maxim/Nero/rfirst.csv\"\n",
    "    csv_writer(results1,path)\n",
    "\n",
    "    # Чтение файлов и разбивка\n",
    "    concat_data = pd.read_csv(r\"C:\\Users\\Users\\Maxim\\Nero\\rfirst.csv\", header=0, delimiter=\"\\t\", quoting=3, encoding = \"ISO-8859-1\")\n",
    "\n",
    "    train = concat_data.sample(frac=0.0)\n",
    "    test = concat_data.loc[~concat_data.index.isin(train.index)]\n",
    "    train = train.reset_index(drop=True)\n",
    "    test = test.reset_index(drop=True)\n",
    "\n",
    "    train.to_csv( \"C:/Users/Users/Maxim/Nero/TwitterAnalyse/twit2vec/trainlol.csv\", index=False, sep='\\t', escapechar='\\\\', quoting=3 )\n",
    "    test.to_csv( \"C:/Users/Users/Maxim/Nero/TwitterAnalyse/twit2vec/testlol.csv\", index=False, sep='\\t', escapechar='\\\\', quoting=3 )\n",
    "\n",
    "    colmn_review =\"sentiment\"\n",
    "\n",
    "    # Verify the number of reviews that were read (100,000 in total)\n",
    "    #printprint (\"Read %d labeled train reviews, %d labeled test reviews, \" % (train[colmn_review].size, test[colmn_review].size))\n",
    "\n",
    "    # Load the punkt tokenizer\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "\n",
    "    sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "    #print (\"Parsing sentences from training set\")\n",
    "    for review in train[colmn_review]:\n",
    "        sentences += review_to_sentences(review, tokenizer)\n",
    "\n",
    "\n",
    "    # Import the built-in logging module and configure it so that Word2Vec\n",
    "    # creates nice output messages\n",
    "\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "        level=logging.INFO)\n",
    "\n",
    "    # Set values for various parameters\n",
    "    num_features = 300    # Word vector dimensionality\n",
    "    min_word_count = 10   # Minimum word count\n",
    "    num_workers = 4       # Number of threads to run in parallel\n",
    "    context = 15          # Context window size\n",
    "    downsampling = 1e-4   # Downsample setting for frequent words\n",
    "\n",
    "\n",
    "    # ****************************************************************\n",
    "    model=word2vec.Word2Vec.load(\"C:/Users/Users/Maxim/Nero/100features_10minwords_10context\")\n",
    "    #print(model)\n",
    "    # ****************************************************************\n",
    "    # Calculate average feature vectors for training and testing sets,\n",
    "    # using the functions we defined above. Notice that we now use stop word\n",
    "    # removal.\n",
    "\n",
    "    clean_train_reviews = []\n",
    "    for review in train[colmn_review]:\n",
    "        clean_train_reviews.append( review_to_wordlist( review,remove_stopwords=True ))\n",
    "\n",
    "\n",
    "    count_null_nwords=0\n",
    "    trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )\n",
    "    #print('count_null_nwords: ', count_null_nwords)\n",
    "\n",
    "\n",
    "    #print (\"Creating average feature vecs for test reviews\")\n",
    "    clean_test_reviews = []\n",
    "    for review in test[colmn_review]:\n",
    "        clean_test_reviews.append( review_to_wordlist( review, remove_stopwords=True ))\n",
    "\n",
    "    testDataVecs = getAvgFeatureVecs( clean_test_reviews, model, num_features )\n",
    "\n",
    "\n",
    "    forest = RandomForestClassifier( n_estimators = 100 )\n",
    "\n",
    "    # load it again\n",
    "    with open('C:/Users/Users/Maxim/Nero/my_dumped_classifier.pkl', 'rb') as fid:\n",
    "        forest = cPickle.load(fid)\n",
    "\n",
    "    # Test & extract results\n",
    "    result = forest.predict( testDataVecs )\n",
    "\n",
    "    # Write the test results\n",
    "    output = pd.DataFrame( data={\"id\":test[\"id_tweet\"], \"review\":result,\"us_id\":test[\"us_id\"],\"followers\":test[\"followers\"]} )\n",
    "    output.to_csv(\"C:/Users/Users/Maxim/Nero/TwitterAnalyse/twit2vec/Word2Vec_AverageVectors_r.csv\", index=False, quoting=3)\n",
    "    #print(\"OK\")\n",
    "\n",
    "    #КОНЕЦ ПЕРВОЙ НЕЙРОСЕТИ\n",
    "\n",
    "    # Создаем курсор - это специальный объект который делает запросы и получает их результаты\n",
    "    cursor3 = conn.cursor()\n",
    "    cursor4 = conn.cursor()\n",
    "    cursor5 = conn.cursor()\n",
    "\n",
    "    fmt = '%Y-%m-%d %H:%M:%S'\n",
    "    fmt2 = '%d.%m.%Y %H:%M:%S'\n",
    "\n",
    "    # Делаем SELECT запрос к базе данных, используя обычный SQL-синтаксис\n",
    "    results3 = cursor3.fetchall()\n",
    "    ZZZ=0\n",
    "\n",
    "    \n",
    "    #ДАННЫЕ ИЗ ПЕРВОЙ НЕЙРОСЕТИ В БД\n",
    "    cursor3 = conn.cursor()\n",
    "    cursor3.execute(\"\"\"DELETE FROM rner1\"\"\")\n",
    "    conn.commit()\n",
    "    z=0\n",
    "    with open('C:/Users/Users/Maxim/Nero/TwitterAnalyse/twit2vec/Word2Vec_AverageVectors_r.csv', encoding=\"utf8\") as tsvfile:\n",
    "        reader = csv.reader(tsvfile, delimiter=',')\n",
    "        for row in reader:\n",
    "            if(z>0):\n",
    "                cursor5.execute(\"insert into rner1 (followers,id_tweet,review,us_id)values (?,?,?,?)\",\n",
    "                        (row[0],row[1],row[2],row[3]))\n",
    "            z+=1\n",
    "    conn.commit()\n",
    "\n",
    "    \n",
    "    #Создаю массив для ввода во вторую нейросеть\n",
    "    cursor4.execute(\"\"\"SELECT * FROM rner1\"\"\")\n",
    "    results4 = cursor4.fetchall()\n",
    "    a=0\n",
    "    i=0\n",
    "    o=0\n",
    "    Z = np.zeros((2999,1))\n",
    "    for i in range(len(results4)):\n",
    "        while a<1499:\n",
    "            Z[2*a]=results4[a][1]\n",
    "            Z[2*a+1]=results4[a][3]\n",
    "            a+=1\n",
    "\n",
    "    Z=Z.T\n",
    "    X_test = Z\n",
    "\n",
    "    \n",
    "    #ПРОВЕРЯЮ РЕЗУЛЬТАТ\n",
    "    x = model_load.predict(X_test)\n",
    "    for i in range(len(x)):\n",
    "        print(\"X=%s, Predicted=%s\" % (X_test[i], x[i]))\n",
    "        #print(datetime.utcnow())\n",
    "        y = x[i]\n",
    "    if y[0]==1:\n",
    "        r = 1\n",
    "    elif y[1]==1:\n",
    "        r=0\n",
    "    else:\n",
    "        r=-1\n",
    "        \n",
    "    return(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем соединение с нашей базой данных\n",
    "# В нашем примере у нас это просто файл базы\n",
    "conn = sqlite3.connect('C:/Users/Users/Maxim/Nero/Chinook_Sqlite.sqlite')\n",
    "\n",
    "# Создаем курсор - это специальный объект который делает запросы и получает их результаты\n",
    "cursor1 = conn.cursor()\n",
    "cursor1.execute(\"\"\"SELECT id_tweet, review, us_id, followers FROM rtwi ORDER BY id_tweet DESC LIMIT 1500\"\"\")\n",
    "results1 = cursor1.fetchall()\n",
    "\n",
    "\n",
    "path_save_itog = 'C:/Users/Users/Maxim/Nero/itog1.csv'\n",
    "url111 = 'https://coinmarketcap.com/ru/currencies/bitcoin/'\n",
    "\n",
    "# ML model\n",
    "model_load = load_model('C:/Users/Users/Maxim/Nero/my_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-01 12:27:00,843 : INFO : loading Word2Vec object from C:/Users/Users/Maxim/Nero/100features_10minwords_10context\n",
      "2018-11-01 12:27:01,366 : INFO : loading wv recursively from C:/Users/Users/Maxim/Nero/100features_10minwords_10context.wv.* with mmap=None\n",
      "2018-11-01 12:27:01,368 : INFO : setting ignored attribute vectors_norm to None\n",
      "2018-11-01 12:27:01,369 : INFO : loading vocabulary recursively from C:/Users/Users/Maxim/Nero/100features_10minwords_10context.vocabulary.* with mmap=None\n",
      "2018-11-01 12:27:01,371 : INFO : loading trainables recursively from C:/Users/Users/Maxim/Nero/100features_10minwords_10context.trainables.* with mmap=None\n",
      "2018-11-01 12:27:01,373 : INFO : setting ignored attribute cum_table to None\n",
      "2018-11-01 12:27:01,375 : INFO : loaded C:/Users/Users/Maxim/Nero/100features_10minwords_10context\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:311: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.19.1 when using version 0.19.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:311: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.19.1 when using version 0.19.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X=[1.01933199e+18 1.05789673e+18 1.01933199e+18 ... 1.09423855e+09\n",
      " 1.05789434e+18 0.00000000e+00], Predicted=[1. 0. 0.]\n",
      "1,2018-11-01 12:27:06.076759,6403.97\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    \n",
    "    page111 = requests.get(url111)\n",
    "    soup11 = BeautifulSoup(page111.content, 'html.parser')\n",
    "    ART = soup11.find('span', class_=\"h2 text-semi-bold details-panel-item--price__value\")\n",
    "    price111 = ART.string\n",
    "    dnnn = str(lol()) + ',' + str(datetime.now()) + ',' + str(price111)\n",
    "    csv_writer1(dnnn, path_save_itog)\n",
    "    print(dnnn)\n",
    "\n",
    "    time.sleep(1500)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
